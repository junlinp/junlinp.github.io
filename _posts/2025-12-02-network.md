---
layout: post
title: "如何训练好一个网络"
date: 2024-12-01 10:00:00 -0000
categories: technical
---

我通常有一个疑问：为什么现在有这么多的网络架构，只要模型的capacity大于数据的分布，大力出奇迹不就好了。
当自己训练的时候，发现，好像并不是这么回事。

在训练BEV to trajectory的时候，尝试过不同layer数目的transformer架构的网络。
对于小数据，layer小的的网络，能非常快的收敛，随着网络层数的提高，我相信它肯定能收敛，但是epoch需要的数目就需要非常地多。

了解了一下scale law,感觉就是个水多加面，面多加水的过程.
对于对网络和数据都没任何sense的情况下，如何评估训练时间足够长？
先小数据小网络
- 训练足够长的时间，模型loss足够低 -> 模型容量够，加数据，回到开始.
- 训练足够长的时间，模型loss不够低 -> 模型容量不够，加网络，回到开始.

做了一个对scale law的小实验。训练了一个小的world model。
用gymnasium使用random action去生成了1000 rollout.跑1000 epoch

模型:
- GRU encoder layer with layer number 1 +  GRU decoder layer with layer number 1 + MLP
- GRU encoder layer with layer number 4 + GRU decoder layer with layer number 4 + MLP

![World Model](/assets/world_model.jpg)

summary:
- 大模型在最开始的下降阶段，能用更少的step达到小模型的相同的精度。
- 但是在loss平稳下降的过程中，大模型的loss比小模型的loss大
- 大概就是500 epoch附近,大模型和小模型的loss相近
- 最终大模型的loss小于了小模型，大约就是0.025 vs 0.050
- CPU消耗时长, 小模型vs大模型 = 1：3.x ~ 4


要了解数据的分布

BEV to trajectory.发现生成的轨迹在gt附近会存在大范围的抖动，然后简单观察了一下x, y的数据分布.

$$x \in [0, 20+], \quad y \in [-20, +20], \quad z \in [-0.5, 0.5]$$

网络我的输出是直接输出一个real value.

要让模型的输出去拟合 20.+的数值，同时让误差接近0.01,要精确到4位有效数字，用float32的话,只有7位左右的有效数字，有点难为网络了.

于是我将数值离散话，[-40, 40] 离散化到0.01的resolution.即转化为是8192个类型的分类问题.

15k步达到之前60k步的效果，同时目前loss还在肉眼可见地下降.

"entropy loss is all you need"...


![Traj](/assets/network/20251204-195617.jpg)
![Loss](/assets/network/20251204-195628.jpg)
相当好看的曲线

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>

