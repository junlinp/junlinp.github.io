---
layout: post
title: "如何训练好一个网络"
date: 2024-12-01 10:00:00 -0000
categories: technical
---

我通常有一个疑问：为什么现在有这么多的网络架构，只要模型的capacity大于数据的分布，大力出奇迹不就好了。

当自己训练的时候，发现，好像并不是这么回事。

在训练BEV to trajectory的时候，尝试过不同layer数目的transformer架构的网络。对于小数据，layer小的的网络，能非常快的收敛，随着网络层数的提高，我相信它肯定能收敛，但是epoch需要的数目就需要非常地多。
如果需要做不同的实验，就需要许多的卡来并发训练不同的架构和参数。

了解了一下scale law,感觉就是个水多加面，面多加水的过程.

对于对网络和数据都没任何sense的情况下，如何评估训练时间足够长？

先小数据小网络
- 训练足够长的时间，模型loss足够低 -> 模型容量够，加数据，回到开始.
- 训练足够长的时间，模型loss不够低 -> 模型容量不够，加网络，回到开始.

本质就是拿算力去实验?

感觉网络架构的不同，就是为了可以人为加约束,缩小网络的函数搜索空间.



