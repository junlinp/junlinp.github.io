---
layout: post
title: "如何训练好一个网络"
date: 2024-12-01 10:00:00 -0000
categories: technical
---

我通常有一个疑问：为什么现在有这么多的网络架构，只要模型的capacity大于数据的分布，大力出奇迹不就好了。

当自己训练的时候，发现，好像并不是这么回事。

在训练BEV to trajectory的时候，尝试过不同layer数目的transformer架构的网络。对于小数据，layer小的的网络，能非常快的收敛，随着网络层数的提高，我相信它肯定能收敛，但是epoch需要的数目就需要非常地多。
如果需要做不同的实验，就需要许多的卡来并发训练不同的架构和参数。

了解了一下scale law,感觉就是个水多加面，面多加水的过程.

对于对网络和数据都没任何sense的情况下，如何评估训练时间足够长？

先小数据小网络
- 训练足够长的时间，模型loss足够低 -> 模型容量够，加数据，回到开始.
- 训练足够长的时间，模型loss不够低 -> 模型容量不够，加网络，回到开始.

本质就是拿算力去实验?

感觉网络架构的不同，就是为了可以人为加约束,缩小网络的函数搜索空间.


做了一个对scale law的小实验。训练了一个小的world model。
用gymnasium使用random action去生成了1000 rollout.跑1000 epoch

模型:
- GRU encoder layer with layer number 1 +  GRU decoder layer with layer number 1 + MLP
- GRU encoder layer with layer number 2 + GRU decoder layer with layer number 2 + MLP

![World Model](assets/world_model.jpg)

summary:
- 大模型在最开始的下降阶段，能用更少的step达到小模型的相同的精度。
- 但是在loss平稳下降的过程中，大模型的loss比小模型的loss大
- 大概就是500 epoch附近,大模型和小模型的loss相近
- 最终大模型的loss小于了小模型，大约就是0.025 vs 0.050
- CPU消耗时长, 小模型vs大模型 = 1：3.x ~ 4




